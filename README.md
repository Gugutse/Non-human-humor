# Surely you're joking, ruGPT-3!

Сегодня мы будем сочинять стендап с помощью нейросетей. И это не шутки!

Недавно в одном университете ruGPT-3 дообучили на анекдотах -- и вот, например, что она теперь рассказывает:
> Забегает в бар программист и заказывает ящерицу в стакане. Заказывает 0 ящериц в стакане. Заказывает 999999999 ящериц в стакане. Заказывает -1 ящерицу в стакане. Заказывает САГАРОНЕДЦЫБ. Заказывает ФАОЛФВОЫЛ.

Не очень забавляет? Может быть, у тебя получится лучше?

## Задача
Необходимо сгенерировать набор связных шуток на любые темы. 

Итоговый текст оценивается по следующим критериям:

* __Юмор__. Понятие субъективное, но мы постараемся быть как можно менее предвзяты в оценке. Если говорить более формально, важно, чтобы полученные тексты содержали в себе типичные элементы шутки/байки/анекдота -- иронию, сарказм, игру слов, сетапы и панчлайны; основывались на актуальных новостях, культурном контексте и т. д. 
* __Стилевая целостность__. Будет здорово, если все получившиеся шутки будут связаны тематически, и потенциально могли бы стать частью одного стендап-концерта. Кроме того, вы можете сконцентрироваться на шутках определенного комика, и генерировать тексты в его стиле. 

## Baseline

В [ноутбуке](https://github.com/Gugutse/Non-human-humor/blob/main/Surely_you_are_joking%2C_ruGPT_3.ipynb) вы можете найти пример генерации коротких забавных текстов с нуля на русском языке с помощью модели ruGPT-3 от Сбера. В качестве обучающих данных используется [датасет русскоязычных шуток](https://github.com/computational-humor/humor-recognition/tree/master/data)<sup>1</sup> (10 тыс. примеров из ~126 тыс. текстов, определенных как "шутка").


## Данные
Здесь мы не будем ограничивать вашу фантазию -- можете использовать твиты вашего любимого стендап-комика, архивы анекдотов про Вовочку, сценарии текстов Монти Пайтона или "Вечернего Урганта" -- всё, что кажется вам смешным, может пойти в ход. Не стесняйтесь экспериментировать! 


## Ссылки
Узнать подробности про ruGPT-3 можно в [блоге](https://habr.com/ru/company/sberbank/blog/528966/) и на официальном [гитхабе](https://github.com/sberbank-ai/ru-gpts) проекта.

Почитать про fine-tuning модели можно, например, [здесь](https://towardsdatascience.com/fine-tune-a-non-english-gpt-2-model-with-huggingface-9acc2dc7635b) и [здесь](https://towardsdatascience.com/fine-tuning-gpt2-for-text-generation-using-pytorch-2ee61a4f1ba7).

Генерация коротких шуток с помощью трансформеров также описана в [статье](https://pmbaumgartner.github.io/blog/gpt2-jokes/).
Для вдохновения можно оценить [сборник GPT-творчества на английском](https://www.gwern.net/GPT-3#humor) или [код для генерации текстов в стилистике Жириновского](https://github.com/GraphGrailAi/ruGPT3-ZhirV).




> 1. Blinov et al: Large Dataset and Language Model Fun-tuning for Humor Recognition // ACL, (2019).
